{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import time\n",
    "import random\n",
    "\n",
    "import pickle\n",
    "\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Crawling data from Memecenter website\n",
    "\n",
    "baseUrl = 'http://www.memecenter.com/hall/{year}/{month}'\n",
    "\n",
    "years = range(2013,2017)\n",
    "months = ['january','february','march','april','may','june',\n",
    "          'july','august','september','october','november','december']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crawling http://www.memecenter.com/hall/2013/january\n",
      "Stored in memecenter/2013-january.html\n",
      "Crawling http://www.memecenter.com/hall/2013/february\n",
      "Stored in memecenter/2013-february.html\n",
      "Crawling http://www.memecenter.com/hall/2013/march\n",
      "Stored in memecenter/2013-march.html\n",
      "Crawling http://www.memecenter.com/hall/2013/april\n",
      "Stored in memecenter/2013-april.html\n",
      "Crawling http://www.memecenter.com/hall/2013/may\n",
      "Stored in memecenter/2013-may.html\n",
      "Crawling http://www.memecenter.com/hall/2013/june\n",
      "Stored in memecenter/2013-june.html\n",
      "Crawling http://www.memecenter.com/hall/2013/july\n",
      "Stored in memecenter/2013-july.html\n",
      "Crawling http://www.memecenter.com/hall/2013/august\n",
      "Stored in memecenter/2013-august.html\n",
      "Crawling http://www.memecenter.com/hall/2013/september\n",
      "Stored in memecenter/2013-september.html\n",
      "Crawling http://www.memecenter.com/hall/2013/october\n",
      "Stored in memecenter/2013-october.html\n",
      "Crawling http://www.memecenter.com/hall/2013/november\n",
      "Stored in memecenter/2013-november.html\n",
      "Crawling http://www.memecenter.com/hall/2013/december\n",
      "Stored in memecenter/2013-december.html\n",
      "Crawling http://www.memecenter.com/hall/2014/january\n",
      "Stored in memecenter/2014-january.html\n",
      "Crawling http://www.memecenter.com/hall/2014/february\n",
      "Stored in memecenter/2014-february.html\n",
      "Crawling http://www.memecenter.com/hall/2014/march\n",
      "Stored in memecenter/2014-march.html\n",
      "Crawling http://www.memecenter.com/hall/2014/april\n",
      "Stored in memecenter/2014-april.html\n",
      "Crawling http://www.memecenter.com/hall/2014/may\n",
      "Stored in memecenter/2014-may.html\n",
      "Crawling http://www.memecenter.com/hall/2014/june\n",
      "Stored in memecenter/2014-june.html\n",
      "Crawling http://www.memecenter.com/hall/2014/july\n",
      "Stored in memecenter/2014-july.html\n",
      "Crawling http://www.memecenter.com/hall/2014/august\n",
      "Stored in memecenter/2014-august.html\n",
      "Crawling http://www.memecenter.com/hall/2014/september\n",
      "Stored in memecenter/2014-september.html\n",
      "Crawling http://www.memecenter.com/hall/2014/october\n",
      "Stored in memecenter/2014-october.html\n",
      "Crawling http://www.memecenter.com/hall/2014/november\n",
      "[ERROR]: HTTP Error 404: Not Found\n",
      "Crawling http://www.memecenter.com/hall/2014/december\n",
      "Stored in memecenter/2014-december.html\n",
      "Crawling http://www.memecenter.com/hall/2015/january\n",
      "Stored in memecenter/2015-january.html\n",
      "Crawling http://www.memecenter.com/hall/2015/february\n",
      "Stored in memecenter/2015-february.html\n",
      "Crawling http://www.memecenter.com/hall/2015/march\n",
      "Stored in memecenter/2015-march.html\n",
      "Crawling http://www.memecenter.com/hall/2015/april\n",
      "Stored in memecenter/2015-april.html\n",
      "Crawling http://www.memecenter.com/hall/2015/may\n",
      "Stored in memecenter/2015-may.html\n",
      "Crawling http://www.memecenter.com/hall/2015/june\n",
      "Stored in memecenter/2015-june.html\n",
      "Crawling http://www.memecenter.com/hall/2015/july\n",
      "Stored in memecenter/2015-july.html\n",
      "Crawling http://www.memecenter.com/hall/2015/august\n",
      "Stored in memecenter/2015-august.html\n",
      "Crawling http://www.memecenter.com/hall/2015/september\n",
      "Stored in memecenter/2015-september.html\n",
      "Crawling http://www.memecenter.com/hall/2015/october\n",
      "Stored in memecenter/2015-october.html\n",
      "Crawling http://www.memecenter.com/hall/2015/november\n",
      "Stored in memecenter/2015-november.html\n",
      "Crawling http://www.memecenter.com/hall/2015/december\n",
      "Stored in memecenter/2015-december.html\n",
      "Crawling http://www.memecenter.com/hall/2016/january\n",
      "Stored in memecenter/2016-january.html\n",
      "Crawling http://www.memecenter.com/hall/2016/february\n",
      "Stored in memecenter/2016-february.html\n",
      "Crawling http://www.memecenter.com/hall/2016/march\n",
      "Stored in memecenter/2016-march.html\n",
      "Crawling http://www.memecenter.com/hall/2016/april\n",
      "Stored in memecenter/2016-april.html\n",
      "Crawling http://www.memecenter.com/hall/2016/may\n",
      "Stored in memecenter/2016-may.html\n",
      "Crawling http://www.memecenter.com/hall/2016/june\n",
      "Stored in memecenter/2016-june.html\n",
      "Crawling http://www.memecenter.com/hall/2016/july\n",
      "Stored in memecenter/2016-july.html\n",
      "Crawling http://www.memecenter.com/hall/2016/august\n",
      "Stored in memecenter/2016-august.html\n",
      "Crawling http://www.memecenter.com/hall/2016/september\n",
      "Stored in memecenter/2016-september.html\n",
      "Crawling http://www.memecenter.com/hall/2016/october\n",
      "Stored in memecenter/2016-october.html\n",
      "Crawling http://www.memecenter.com/hall/2016/november\n",
      "Stored in memecenter/2016-november.html\n",
      "Crawling http://www.memecenter.com/hall/2016/december\n",
      "Stored in memecenter/2016-december.html\n"
     ]
    }
   ],
   "source": [
    "def crawl_single_page(pUrl):\n",
    "    print ('Crawling {}'.format(pUrl))\n",
    "    opener = urllib.request.build_opener()\n",
    "    # We add header because some websites check for this before responding to request\n",
    "    opener.addheaders = [('User-agent', 'Mozilla/5.0')]\n",
    "    \n",
    "    try: # If there is any error, report error message and continue crawling\n",
    "        # We create file name based on url pattern\n",
    "        # Make sure memecenter folder exists!!!\n",
    "        fname = 'memecenter/{}.html'.format(pUrl.replace('/','-').split('hall-')[-1])\n",
    "\n",
    "        # Writing to a file\n",
    "        with open(fname,'w') as fOut:\n",
    "            # Making a request for html content\n",
    "            html = opener.open(pUrl).read().decode('utf-8')\n",
    "            fOut.write(html)\n",
    "        print ('Stored in {}'.format(fname))\n",
    "    except Exception as e:\n",
    "        print ('[ERROR]: {}'.format(e))\n",
    "    \n",
    "# Iterate over years and months\n",
    "for y in years:\n",
    "    for m in months:\n",
    "        crawl_single_page(baseUrl.format(year=y, month=m))\n",
    "        time.sleep(random.random())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with crawled data\n",
    "\n",
    "So far we crawled all the data and stored them under `memecenter` folder. \n",
    "\n",
    "Once data is all crawled, we can attempt to parse it multiple times. It's important to crawl data first before analysis.\n",
    "\n",
    "Let's read those stored data and try to parse it.\n",
    "\n",
    "## Searching and parsing information\n",
    "\n",
    "To search information, I usually use [developer tools](https://developer.chrome.com/devtools) to inspect the html content (all good browsers Firefox, Chrome has it. I don't know about Microsoft). \n",
    "\n",
    "I recommend the same for you. It's easier to find information about tags, location and available data. Sometimes what you see in the webpage is not all the data in the html and you can obtain more by investigating these.\n",
    "\n",
    "Improve your code step-by-step. First find the outermost element and print if you get the right content. Then try to parse inner elements.\n",
    "\n",
    "For instance: Facebook images has alt-tag for additional metadata. It provides list of objects detected in the image. [see](https://github.com/ageitgey/show-facebook-computer-vision-tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing memecenter/2013-january.html\n",
      "20 item collected from page\n",
      "Parsing memecenter/2013-february.html\n",
      "20 item collected from page\n",
      "Parsing memecenter/2013-march.html\n",
      "20 item collected from page\n",
      "Parsing memecenter/2013-april.html\n",
      "20 item collected from page\n",
      "Parsing memecenter/2013-may.html\n",
      "20 item collected from page\n",
      "Parsing memecenter/2013-june.html\n",
      "20 item collected from page\n",
      "Parsing memecenter/2013-july.html\n",
      "20 item collected from page\n",
      "Parsing memecenter/2013-august.html\n",
      "20 item collected from page\n",
      "Parsing memecenter/2013-september.html\n",
      "20 item collected from page\n",
      "Parsing memecenter/2013-october.html\n",
      "20 item collected from page\n",
      "Parsing memecenter/2013-november.html\n",
      "20 item collected from page\n",
      "Parsing memecenter/2013-december.html\n",
      "20 item collected from page\n",
      "Parsing memecenter/2014-january.html\n",
      "20 item collected from page\n",
      "Parsing memecenter/2014-february.html\n",
      "20 item collected from page\n",
      "Parsing memecenter/2014-march.html\n",
      "20 item collected from page\n",
      "Parsing memecenter/2014-april.html\n",
      "20 item collected from page\n",
      "Parsing memecenter/2014-may.html\n",
      "20 item collected from page\n",
      "Parsing memecenter/2014-june.html\n",
      "20 item collected from page\n",
      "Parsing memecenter/2014-july.html\n",
      "20 item collected from page\n",
      "Parsing memecenter/2014-august.html\n",
      "20 item collected from page\n",
      "Parsing memecenter/2014-september.html\n",
      "20 item collected from page\n",
      "Parsing memecenter/2014-october.html\n",
      "20 item collected from page\n",
      "Parsing memecenter/2014-november.html\n",
      "[ERROR]: 'NoneType' object has no attribute 'find_all'\n",
      "Parsing memecenter/2014-december.html\n",
      "20 item collected from page\n",
      "Parsing memecenter/2015-january.html\n",
      "20 item collected from page\n",
      "Parsing memecenter/2015-february.html\n",
      "20 item collected from page\n",
      "Parsing memecenter/2015-march.html\n",
      "20 item collected from page\n",
      "Parsing memecenter/2015-april.html\n",
      "20 item collected from page\n",
      "Parsing memecenter/2015-may.html\n",
      "20 item collected from page\n",
      "Parsing memecenter/2015-june.html\n",
      "20 item collected from page\n",
      "Parsing memecenter/2015-july.html\n",
      "20 item collected from page\n",
      "Parsing memecenter/2015-august.html\n",
      "20 item collected from page\n",
      "Parsing memecenter/2015-september.html\n",
      "20 item collected from page\n",
      "Parsing memecenter/2015-october.html\n",
      "20 item collected from page\n",
      "Parsing memecenter/2015-november.html\n",
      "20 item collected from page\n",
      "Parsing memecenter/2015-december.html\n",
      "20 item collected from page\n",
      "Parsing memecenter/2016-january.html\n",
      "20 item collected from page\n",
      "Parsing memecenter/2016-february.html\n",
      "20 item collected from page\n",
      "Parsing memecenter/2016-march.html\n",
      "20 item collected from page\n",
      "Parsing memecenter/2016-april.html\n",
      "20 item collected from page\n",
      "Parsing memecenter/2016-may.html\n",
      "20 item collected from page\n",
      "Parsing memecenter/2016-june.html\n",
      "20 item collected from page\n",
      "Parsing memecenter/2016-july.html\n",
      "20 item collected from page\n",
      "Parsing memecenter/2016-august.html\n",
      "20 item collected from page\n",
      "Parsing memecenter/2016-september.html\n",
      "20 item collected from page\n",
      "Parsing memecenter/2016-october.html\n",
      "20 item collected from page\n",
      "Parsing memecenter/2016-november.html\n",
      "20 item collected from page\n",
      "Parsing memecenter/2016-december.html\n",
      "20 item collected from page\n"
     ]
    }
   ],
   "source": [
    "def parse_page(fname):\n",
    "    print ('Parsing {}'.format(fname))\n",
    "    # Let's store all the necessary information in this dictionary\n",
    "    pageData = dict()\n",
    "    pageData['filename'] = fname\n",
    "    pageData['topList'] = list()\n",
    "    \n",
    "    html_doc = open(fname,'r').read()\n",
    "    soup = BeautifulSoup(html_doc, 'html.parser') # Parse html content using BeautifulSoup\n",
    "    \n",
    "    contentList = soup.find(id='fdc_contcontainer')\n",
    "    for memeDiv in contentList.find_all('div', {'class':'content'}):\n",
    "        tempData = dict()\n",
    "        tempData['title'] = memeDiv.find('div', {'class':'content-title'}).text\n",
    "        tempData['img-src'] = memeDiv.find('img')['src']\n",
    "        tempData['rank'] = memeDiv.find('div', {'class':'hall_badge'}).text.strip()\n",
    "        \n",
    "        #print (memeDiv)\n",
    "        #print (memeDiv.find('div', {'class':'content-title'}).text)\n",
    "        #print (memeDiv.find('img')['src'])\n",
    "        #print (memeDiv.find('div', {'class':'hall_badge'}).text.strip())\n",
    "        \n",
    "        buttonsDiv = memeDiv.find('div', {'class':'buttons'})\n",
    "        tempData['nLike'] = int(buttonsDiv.find('div', {'class':'like'}).text.strip().replace('Like',''))\n",
    "        tempData['nComment'] = int(buttonsDiv.find('div', {'class':'comment'}).text.strip().replace('Show ','').replace('comments',''))\n",
    "        \n",
    "        #print (buttonsDiv.find('div', {'class':'like'}).text.strip().replace('Like',''))\n",
    "        #print (buttonsDiv.find('div', {'class':'comment'}).text.strip().replace('Show ','').replace('comments',''))\n",
    "        pageData['topList'].append(tempData)\n",
    "    \n",
    "    print ('{} item collected from page'.format(len(pageData['topList'])))\n",
    "    return pageData\n",
    "\n",
    "memeDataset = dict()\n",
    "for y in years:\n",
    "    for m in months:\n",
    "        fname = 'memecenter/{}-{}.html'.format(y,m)\n",
    "        try:\n",
    "            pData = parse_page(fname)\n",
    "        except Exception as e:\n",
    "            print ('[ERROR]: {}'.format(e))\n",
    "        memeDataset['{}-{}'.format(y,m)] = pData\n",
    "        \n",
    "# Store extracted information into a file. Later we can easily load it without repeating preprocessing.\n",
    "#pickle.dump(memeDataset, open('memecenter/page_data.pkl','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load crawled data and analyze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2014-july {'topList': [{'rank': '1', 'nComment': 220, 'nLike': 7310, 'img-src': 'http://img.memecdn.com/dat-feeling_c_1769367.jpg', 'title': 'Dat Feeling'}, {'rank': '2', 'nComment': 161, 'nLike': 5233, 'img-src': 'http://img.memecdn.com/wars_o_678135.jpg', 'title': 'Wars....'}, {'rank': '3', 'nComment': 156, 'nLike': 5054, 'img-src': 'http://img.memecdn.com/never-mind-me-d_o_1646029.jpg', 'title': 'Never Mind Me'}, {'rank': '4', 'nComment': 153, 'nLike': 4470, 'img-src': 'http://img.memecdn.com/we-could-all-use-it_o_2070627.jpg', 'title': 'We Could All Use It.'}, {'rank': '5', 'nComment': 173, 'nLike': 4407, 'img-src': 'http://img.memecdn.com/what-if-he-is-actually-right_o_2516749.jpg', 'title': 'What If He Is Actually Right? '}, {'rank': '6', 'nComment': 113, 'nLike': 4346, 'img-src': 'http://img.memecdn.com/gandalf-the-homeless_o_2470885.jpg', 'title': 'Gandalf The Homeless. '}, {'rank': '7', 'nComment': 132, 'nLike': 4245, 'img-src': 'http://img.memecdn.com/epic-mortal-kombat-cosplay_c_273305.jpg', 'title': 'Epic Mortal Kombat Cosplay'}, {'rank': '8', 'nComment': 91, 'nLike': 4152, 'img-src': 'http://img.memecdn.com/turn-her-into-chicken_c_1074963.jpg', 'title': 'Turn Her Into Chicken'}, {'rank': '9', 'nComment': 57, 'nLike': 4112, 'img-src': 'http://static.memecdn.com/images/hall_normal.png', 'title': 'That Feeling Of Loneliness...'}, {'rank': '10', 'nComment': 175, 'nLike': 4056, 'img-src': 'http://img.memecdn.com/thank-you-come-again_c_2492565.jpg', 'title': 'Thank You Come Again'}, {'rank': '11', 'nComment': 134, 'nLike': 4002, 'img-src': 'http://img.memecdn.com/damn-you-spiderman_c_2463567.jpg', 'title': 'Damn You Spiderman'}, {'rank': '12', 'nComment': 180, 'nLike': 3953, 'img-src': 'http://static.memecdn.com/images/hall_normal.png', 'title': 'Freddie Mercury The Stripper'}, {'rank': '13', 'nComment': 181, 'nLike': 3904, 'img-src': 'http://img.memecdn.com/as-they-say-karma-amp-039-s-only-a-bitch-if-you-are_c_2509047.jpg', 'title': \"As They Say: Karma's Only A Bitch If You Are\"}, {'rank': '14', 'nComment': 35, 'nLike': 3766, 'img-src': 'http://static.memecdn.com/images/hall_normal.png', 'title': 'Tennis: Cat Style'}, {'rank': '15', 'nComment': 173, 'nLike': 3703, 'img-src': 'http://img.memecdn.com/why-cruel-world-why_c_1113061.jpg', 'title': 'Why Cruel World? Why?'}, {'rank': '16', 'nComment': 133, 'nLike': 3737, 'img-src': 'http://static.memecdn.com/images/hall_normal.png', 'title': 'Unrustling Pill'}, {'rank': '17', 'nComment': 217, 'nLike': 3699, 'img-src': 'http://img.memecdn.com/hoist-the-colors-high_o_2589925.jpg', 'title': 'Hoist The Colors High'}, {'rank': '18', 'nComment': 184, 'nLike': 3990, 'img-src': 'http://img.memecdn.com/tumblr-disney-6_c_3489257.jpg', 'title': 'Tumblr Disney #6'}, {'rank': '19', 'nComment': 54, 'nLike': 3574, 'img-src': 'http://img.memecdn.com/parenting-in-the-amp-039-50s-you-amp-039-re-doing-it-wrong_o_1996159.jpg', 'title': \"Parenting In The '50S, You're Doing It Wrong\"}, {'rank': '20', 'nComment': 133, 'nLike': 3555, 'img-src': 'http://img.memecdn.com/the-slutty-grandma_c_2299345.jpg', 'title': 'The Slutty Grandma'}], 'filename': 'memecenter/2014-july.html'}\n"
     ]
    }
   ],
   "source": [
    "memeDataset = pickle.load(open('memecenter/page_data.pkl','rb'))\n",
    "#print (memeDataset.keys())\n",
    "\n",
    "for k in memeDataset:\n",
    "    print (k, memeDataset[k])\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Possible ideas\n",
    "\n",
    "- Check which memes appear in this top list more than once\n",
    "\n",
    "- What are the some memes that has high number of likes and comments\n",
    "\n",
    "- Temporal changes of like and comments per meme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
